\documentclass{ieeeojies}
\usepackage{svg}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{array}
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{float}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% Define a new level of sectioning
\begin{document}
\title{CRYPTOCURRENCY PREDICTION USING MACHINE LEARNING}

\author{\uppercase{VU MINH HOANG}\authorrefmark{1},
\uppercase{DUONG HUY HOANG\authorrefmark{2}, and TRAN THI MINH CHAU}\authorrefmark{3}}

\address[1]{Faculty of Information Systems, University of Information Technology, 21520244@gm.uit.edu.vn}
\address[1]{Faculty of Information Systems, University of Information Technology, 21522087@gm.uit.edu.vn}
\address[1]{Faculty of Information Systems, University of Information Technology, 21521888@gm.uit.edu.vn}

\markboth
{Author \headeretal: VU MINH HOANG, DUONG HUY HOANG, TRAN THI MINH CHAU}
{Author \headeretal: VU MINH HOANG, DUONG HUY HOANG, TRAN THI MINH CHAU}

\begin{abstract}
\end{abstract}

\begin{keywords}
\end{keywords}

\titlepgskip=-15pt

\maketitle

\section{Introduction}
\label{sec:introduction}
In the volatile world of cryptocurrency trading, predicting price movements has become increasingly vital for both investors and traders. This study aims to address the challenging task of forecasting cryptocurrency prices based on historical data. The primary problem we tackle is to use the past ten days of historical data to predict the price of a cryptocurrency for the subsequent day. To achieve this, we explore a variety of machine learning and statistical forecasting methods. These methods include Linear Regression, Long Short-Term Memory (LSTM), SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous factors), Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU), Stacking Model, Autoregressive Transformer (Automformer), and ARIMA (AutoRegressive Integrated Moving Average). Our dataset encompasses historical data of three major cryptocurrencies: Bitcoin, Ethereum, and Binance. It includes five key attributes: Date, Price, Open, High, Low, Volume, and Change(\%), covering a period from March 1, 2019, to March 1, 2024.

\section{Related Works}
In 2023, the authors \cite{seabe2023forecasting} from Sefako Makgatho Health Sciences University have conducted an experiment using three algorithms which are LSTM, Bi-LSTM, and GRU to predict three different cryptocurrencies which are BTC, ETH and Bi-LSTM. The results suggtest that deep learning models are effective in predicting the cryptocurrency prices. However, in a paper from 2022 of the authors \cite{ye2022stacking} from Shenzhen Institute of Advanced Technology, they have used Stacking Ensemble Deep Learning Model consisting of LSTM and GRU. Throughout the experiment, the stacking ensemble models outperform other models in most cases.


\section{Materials}

\subsection{Dataset}
\begin{table}[h]
\centering
\begin{tabular}{|p{3cm}|p{4cm}|}
\hline
\textbf{Attribute} & \textbf{Explanation} \\ \hline
Date & The specific day on which the data point was recorded. \\ \hline
Price & The closing price of the cryptocurrency on the given date. \\ \hline
Open & The price at which the cryptocurrency started trading on the given date. \\ \hline
High & The highest price reached by the cryptocurrency during the trading day. \\ \hline
Low & The lowest price reached by the cryptocurrency during the trading day. \\ \hline
Vol. & The total volume of the cryptocurrency traded on the given date. \\ \hline
Change\% & The percentage change in the price of the cryptocurrency from the previous day's closing price. \\ \hline
\end{tabular}
\caption{Explanation of Cryptocurrency Data Attributes}
\label{table:crypto_attributes}
\end{table}
\subsection{Preprocess}
\subsection{Descriptive Statistics}
\subsubsection{Binance}
\paragraph{Detail statics}
\begin{center}
\begin{tabular}{|l|r|}
\hline
\textbf{Price} & \textbf{Value} \\
\hline
Count & 1920 \\
\hline
Mean & 229.55 \\
\hline
Std & 184.43 \\
\hline
Min & 9.25 \\
\hline
25\% & 27.03 \\
\hline
50\% (Median) & 247.55 \\
\hline
75\% & 332.11 \\
\hline
Max & 676.56 \\
\hline
Mode & 23.01 \\
\hline
Variance & 34014.46 \\
\hline
Kurtosis & -0.89 \\
\hline
Skewness & 0.33 \\
\hline
\end{tabular}
\end{center}
\paragraph{Visualization}
\includegraphics[scale=0.35]{images/Binance historical plot.png}\\
\includegraphics[scale=0.35]{images/Binance price boxplot.png}
\subsubsection{Bitcoin}
\paragraph{Detail statics}
\begin{center}
\begin{tabular}{|l|r|}
\hline
\textbf{Price} & \textbf{Value} \\
\hline
Count & 1920 \\
\hline
Mean & 229.55 \\
\hline
Std & 184.43 \\
\hline
Min & 9.25 \\
\hline
25\% & 27.03 \\
\hline
50\% (Median) & 247.55 \\
\hline
75\% & 332.11 \\
\hline
Max & 676.56 \\
\hline
Mode & 23.01 \\
\hline
Variance & 34014.46 \\
\hline
Kurtosis & -0.89 \\
\hline
Skewness & 0.33 \\
\hline
\end{tabular}
\end{center}
\paragraph{Visualization}
\includegraphics[scale=0.5]{images/Bitcoin_line.png}\\
\includegraphics[scale=0.5]{images/Bitcoin_boxplot.png}
\subsubsection{Ethereum}
\paragraph{Detail statics}
\begin{center}
\begin{tabular}{|l|r|}
\hline
\textbf{Value} & \textbf{Price} \\
\hline
Count &  1920 \\
\hline
Mean & 1580.44 \\
\hline
Std & 1205.86 \\
\hline
Min & 107.9  \\
\hline
25\% & 268.95 \\
\hline
50\% (Median) & 1622.47 \\
\hline
75\% & 2334.24 \\
\hline
Max & 4808.38 \\
\hline
Mode & 125.24 \\
\hline
Variance & 1454102.49 \\
\hline
Kurtosis & -0.73 \\
\hline
Skewness & 0.46 \\
\hline
\end{tabular}

\end{center}
\paragraph{Visualization}
\includegraphics[scale=0.5]{images/ETH_boxplt.png}\\
\includegraphics[scale=0.5]{images/ETH_Line.png}

\section{Methodology}
\subsection{Long Short Term Memory}
Long Short-Term Memory network is a recurrent neural network (RNN), aimed at dealing with the vanishing gradient problem present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus "long short-term memory".

\begin{align*}
X &= \begin{bmatrix}
    x_t \\
    h_{t-1}
\end{bmatrix} \\
f_t &= \sigma(W_f \cdot X + b_f) \\
i_t &= \sigma(W_i \cdot X + b_i) \\
o_t &= \sigma(W_o \cdot X + b_o) \\
\tilde{C}_t &= \tanh(W_c \cdot [h_{t-1}, x_t] + b_c) \\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\
h_t &= o_t \odot \tanh(C_t)
\end{align*}
Where:\\
\indent \textbullet\ \(x_t\) is the input at time step t.\\
\indent \textbullet\ \(h_{t-1}\) is the hidden state at time step t.\\
\indent \textbullet\ \(C_t\) is the cell state at time step t.\\
\indent \textbullet\ \(i_t\) is the input gate.\\
\indent \textbullet\ \(f_t\) is the forget gate.\\
\indent \textbullet\ \(o_t\) is the output gate.\\
\includegraphics[scale=0.4]{images/Screenshot-from-2021-03-16-15-51-05.png}
\subsection{Seasonal AutoRegressive Integrated Moving Average with eXogenous
factors (SARIMAX)}\\
While ARIMA is famous, it has limitations in handling seasonal fluctuation and external factors, which are common in time series data. SARIMAX model is an abbreviation for Seasonal Autoregressive Integrated Moving Average or Seasonal ARIMA and belongs to the family of the ARIMA model. It is mentioned in that the SARIMAX model is capable of handling time series data that has a single variable along with seasonality. Apart from the hyperparameters for ARIMA, three different hyperparameters namely the AutoRegression (AR), Integrated (I), and Moving Average (MA) for the seasonal part of the series along with one parameter for the period of the seasonality. Just adding X (an exogenous variable) in the SARIMA model makes it SARIMAX. Sarimax is denoted as SARIMAX(p,d,q)(P,D,Q,S) where P,D,Q represents the element of AR, differencing, and MA components respectively, while S represents periodicity of the seasons.
\begin{flalign*}
& (1 - \sum_{i=1}^p \phi_i L^i)(1 - \sum_{i=1}^P \Phi_i L^{iS}) \times (1 - L)(1 - L^S)^D Y_t && \\
& = (1 + \sum_{i=1}^q \theta_i L^i)(1 + \sum_{i=1}^Q \Theta_i L^{iS}) \varepsilon_t + \beta X_t &&
\end{flalign*}
Where: \\
\indent \textbullet\ P: the order of auto-regressive.\\
\indent \textbullet\ Q: the order of moving average.\\
\indent \textbullet\ D: the degree of differencing.\\
\indent \textbullet\ S: the periodicity of the seasons.\\
\indent \textbullet\ \(\Theta\): the seasonal AR paramater.\\
\indent \textbullet\ \(\Phi\): the seasonal MA parameter.\\
\subsection{Recurrent Neural Network (RNN)}\\
A recurrent neural network (RNN) is a type of artificial
neural network that uses sequential data or time series data.
RNN is applied successfully to many problems, especially in the
field of NLP (Natural Language Processing). RNN is
distinguished by their "memory," as they use information from
prior inputs to influence the current input and output. While
traditional deep neural networks assume that inputs and outputs
are independent of each other, the output of recurrent neural
networks depends on the prior elements within the sequence.

\includegraphics[scale=0.4]{images/Recurrent_Neural_Network_508b372642.png}
\subsection{Gated Recurrent Unit}
Gated Recurrent Units (GRUs) are a type of RNN that were introduced by in 2014 as an improvement over the traditional LSTM networks. Like LSTMs, GRUs are designed to be able to process input sequences of arbitrary length and maintain a state that encodes information about the past. However, unlike LSTMs, which use multiple gates and an internal memory cell to control the flow of information, GRUs use a single update gate to decide which information to retain and a reset gate to decide which information to discard.
\begin{align*}
        u_t &= \sigma(W_u [h_{t-1}, x_t]) \\
    r_t &= \sigma(W_r [h_{t-1}, x_t]) \\
    h_t &= (1 - u_t) \ast h_{t-1} + u_t \ast \tanh(W [r_t \ast h_{t-1}, u_t])
\end{align*}


Where: \\
\indent \textbullet \(u_t\) is the update gate.\\
\indent \textbullet \(r_t\) is the reset gate.\\
\indent \textbullet \(h_t\) is the hidden state at time step t .\\
\includegraphics[scale=1]{images/Structure-of-the-gated-recurrent-unit-GRU-recurrent-network.png}
\subsection{Stacking Model}
Stacking or stacked generalization is an ensemble machine learning algorithm. The main idea of stacking model is that we combine multiple weak models to enhance the accuracy of prediction. This algorithm consists of two parts: base model and meta model. Meta model use output from base model to produce the best result.\\ \\
\includesvg[scale=0.08]{images/Stacking architecture.svg}
\subsection{AutoFormer}
AutoFormer is an advanced deep learning model specifically designed for time series forecasting tasks. Unlike traditional Transformer models, AutoFormer addresses some limitations when applied to long-term time series forecasting. The standout feature of AutoFormer is the Auto-Correlation Mechanism (ACM), which helps the model learn repeating patterns in the time series more efficiently.
AutoFormer uses the Transformer architecture but incorporates several key improvements:
\begin{itemize}
    \item Auto-Correlation Mechanism (ACM): This replaces the traditional attention mechanism. ACM can autonomously learn and detect periodic patterns and trends in the time series.
\end{itemize}
\begin{itemize}
    \item Decomposition Block: The model decomposes the time series into two main components: trend and seasonal. This separation improves prediction accuracy by isolating different influencing factors on the time series.
\end{itemize}
\includegraphics[scale=0.35]{images/Autoformer.png}
\subsection{AutoRegressive Integrated
Moving Average (ARIMA)} 
ARIMA is a commonly used statistical/economtric model for forecasting time sereis data. The ARIMA model consists of three parts: auto-regressive (AR), integrated (I) and moving-average (MA).The integrated component represents
the amount of differencing required to transform the series data
into a stationary representation. The auto-regressive represents the relationship between the present value of a time and its previous values, capturing their correlation. 
\begin{equation*}
(1 - \sum_{k=1}^{p} \alpha_k L^k)(1 + L^d)X_t = (1 - \sum_{k=1}^{q} \beta_k L^k) \varepsilon_t
\end{equation*}
Where:\\
\indent \textbullet\ \(X_t\) is the time series.\\
\indent \textbullet\ \(\alpha_k\) are the parameters of autoregressive parts. \\
\indent \textbullet\ \(\beta_k\) are the parameters of the moving average part.\\
\indent \textbullet\ \(\varepsilon_t\) is white noise error.\\
\indent \textbullet\ \(p\) is the order of auto-regressive part.\\
\indent \textbullet\ \(q\) is the order of the moving part.\\
\indent \textbullet\ \(d\) is the degree of differencing.\\

Let L be the lag operator, in the above equation and p,d,q are hyper-parameters over which we optimized.At each time t, we train a model using the price history to predict the price at time t and use the sign of the change in price as a prediction.
\subsection{Linear Regression}
 Linear regression is a statistical model which estimates the linear relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables)
 \[Y=\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_kX_k+\varepsilon\]
Where:\\
	\indent\textbullet\ Y is the dependent variable (Target Variable).\\
	\indent\textbullet\ \(X_1, X_2, \ldots, X_k\) are the independent (explanatory) variables.\\
	\indent\textbullet\ \(\beta_0\) is the intercept term.\\
	\indent\textbullet\ \(\beta_1,..., \beta_k\) are the regression coefficients for the independent variables.\\
	\indent\textbullet\ \(\varepsilon\) is the error term.

 \includegraphics[scale=0.5]{images/Screen-Shot-2017-06-04-at-1.46.51-PM.png}
\section{Evaluation Methods}
\textbf{Mean absolute error} (MAE): 
The Mean Absolute Error is the average of all absolute errors. The formula is:\\
\[MAE = \frac{1}{n} \sum_{i=1}^{n} | y_i - \hat{y}_i |\]\\
  
\textbf{Root mean squared error} (RMSE): The Root Mean Absolute Error is the standard deviation of the residuals (prediction errors). The formula is: \\
\[RMSE=\sqrt{\sum_{i=1}^{n} \frac{(\hat{y_i}-y_i )^2}{n} }\]\\
\textbf{Mean Absolute Percentage Error} (MAPE): The Mean Absolute Percentage Error is a measure of prediction accuracy of a forecasting method. The formula is:  \\
\[MAPE=\frac{1}{n}\sum_{i=1}^{n} \frac{|y_i-\hat{y}_i|}{y_i}\]
Where: \\
	\indent\textbullet\ \(n\) is the number of observations in the dataset.\\
	\indent\textbullet\ \(y_i\)  is the true value.\\
	\indent\textbullet\ \(\hat{y_i}\) is the predicted value.
\section{Result}
\subsection{Models}
\subsubsection{Long Short Term Memory}
\includegraphics[scale=0.3]{images/LSTM_Bitcoin.png}
\subsubsection{Seasonal Autoregressive Integrated
Moving Average With Exogenous Factors
(SARIMAX)}
\subsubsection{Gated recurrent unit}
\includegraphics[scale=0.3]{images/GRU_Bitcoin.png}
\subsubsection{Recurrent Neural Network (RNN)}
\includegraphics[scale=0.3]{images/RNN_Bitcoin.png}
\subsubsection{Stacking Model}
\includegraphics[scale=0.3]{images/SM_Bitcoin.png}
\subsubsection{Autoregressive Transformer}
\subsubsection{Autoregressive Integrated Moving Average (ARIMA)}
\includegraphics[scale=0.3]{images/ARIMA_Bitcoin.png}
\subsubsection{Linear Regression}
\includegraphics[scale=0.3]{images/LR_Bitcoin.png}
\subsection{Evaluation result}
\subsubsection{Binance}
\begin{center}
\begin{tabular}{|p{2cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|}
\hline
&\textbf{MAPE} & \textbf{MAE} & \textbf{RMSE} \\
\hline
LSTM  & & &\\
\hline
SARIMAX & & &\\
\hline
GRU & & &\\
\hline
RNN & & &\\
\hline
Stacking Model & & &\\
\hline
Autoregressive Transformer & & &\\
\hline
ARIMA & & &\\
\hline
Linear Regression &  &  & \\
\hline
\end{tabular}
\end{center}
\subsubsection{Bitcoin}
\begin{center}
\begin{tabular}{|p{2cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|}
\hline
&\textbf{MAPE} & \textbf{MAE} & \textbf{RMSE} \\
\hline
LSTM  & 4.45\%& 1.54\%&0.43\%\\
\hline
SARIMAX & \%& \%&\%\\
\hline
GRU & 1.11\%& 0.36\%& 0.43\%\\
\hline
RNN &  0.55\%& 0.18\% & 0.22\%\\
\hline
Stacking Model & 2.17\% & 0.68\% & 0.86\%\\ 
\hline
Autoregressive Transformer & \% & \% & \%\\
\hline
ARIMA & 51.94\% & 14.08\%  & 17.11\%\\
\hline
Linear Regression & 0.64\% & 0.2\% & 0.32\%\\
\hline
\end{tabular}
\end{center}
\subsubsection{Ethereum}
\begin{center}
\begin{tabular}{|p{2cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|}
\hline
&\textbf{MAPE} & \textbf{MAE} & \textbf{RMSE} \\
\hline
LSTM  & & &\\
\hline
SARIMAX & & &\\
\hline
GRU & & &\\
\hline
RNN & & &\\
\hline
Stacking Model & & &\\ 
\hline
Autoregressive Transformer & & &\\
\hline
ARIMA & & &\\
\hline
Linear Regression & & &\\
\hline
\end{tabular}
\end{center}

\section{Conclusion}
\subsection{Summary}

\subsection{Future Considerations}

\section*{Acknowledgment}
\addcontentsline{toc}{section}{Acknowledgment}
\nocite{*}
\bibliographystyle{IEEEannot}
\bibliography{Ref}
\EOD

\end{document}
