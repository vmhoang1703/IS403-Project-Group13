\documentclass{ieeeojies}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{array}
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{float}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
\title{CRYPTOCURRENCY PREDICTION USING MACHINE LEARNING}

\author{\uppercase{VU MINH HOANG}\authorrefmark{1},
\uppercase{DUONG HUY HOANG\authorrefmark{2}, and TRAN THI MINH CHAU}\authorrefmark{3}}

\address[1]{Faculty of Information Systems, University of Information Technology, 21520244@gm.uit.edu.vn}
\address[1]{Faculty of Information Systems, University of Information Technology, 21522087@gm.uit.edu.vn}
\address[1]{Faculty of Information Systems, University of Information Technology, 21521888@gm.uit.edu.vn}

\markboth
{Author \headeretal: VU MINH HOANG, DUONG HUY HOANG, TRAN THI MINH CHAU}
{Author \headeretal: VU MINH HOANG, DUONG HUY HOANG, TRAN THI MINH CHAU}

\begin{abstract}
\end{abstract}

\begin{keywords}
\end{keywords}

\titlepgskip=-15pt

\maketitle

\section{Introduction}
\label{sec:introduction}
In the volatile world of cryptocurrency trading, predicting price movements has become increasingly vital for both investors and traders. This study aims to address the challenging task of forecasting cryptocurrency prices based on historical data. The primary problem we tackle is to use the past ten days of historical data to predict the price of a cryptocurrency for the subsequent day. To achieve this, we explore a variety of machine learning and statistical forecasting methods. These methods include Linear Regression, Long Short-Term Memory (LSTM), SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous factors), Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU), Stacking Model, Autoregressive Transformer (Automformer), and ARIMA (AutoRegressive Integrated Moving Average). Our dataset encompasses historical data of three major cryptocurrencies: Bitcoin, Ethereum, and Binance. It includes five key attributes: Date, Price, Open, High, Low, Volume, and Change(\%), covering a period from March 1, 2019, to March 1, 2024.

\section{Related Works}



\section{Materials}
\subsection{Dataset}
\begin{table}[H]
\centering
\begin{tabular}{l l}
 \textbf{Feature} & \textbf{Definition} \\ \\
 Date & Date \\ \\
 Open & Opening prices\\ \\
 High & Highest prices during the day \\ \\
 Low & Lowest prices during the day\\ \\
 Volume & Trading volume in the day\\ \\
Price & The closing price\\ \\
 Change(\%) & Percentage change in Bitcoin's price from the previous day\\
\end{tabular}
\end{table}
\subsection{Descriptive Statistics}
\begin{table}[H]
\caption{Bitcoin, Ethereum, Solana Descriptive Statistics}
\end{table}
\section{Methodology}
\subsection{Long Short Term Memory}\\
Long Short-Term Memory network is a recurrent neural network (RNN), aimed at dealing with the vanishing gradient problem present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus "long short-term memory".

\begin{align*}
X &= \begin{bmatrix}
    x_t \\
    h_{t-1}
\end{bmatrix} \\
f_t &= \sigma(W_f \cdot X + b_f) \\
i_t &= \sigma(W_i \cdot X + b_i) \\
o_t &= \sigma(W_o \cdot X + b_o) \\
\tilde{C}_t &= \tanh(W_c \cdot [h_{t-1}, x_t] + b_c) \\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\
h_t &= o_t \odot \tanh(C_t)
\end{align*}
Where:\\
\indent \textbullet\ \(x_t\) is the input at time step t.\\
\indent \textbullet\ \(h_{t-1}\) is the hidden state at time step t.\\
\indent \textbullet\ \(C_t\) is the cell state at time step t.\\
\indent \textbullet\ \(i_t\) is the input gate.\\
\indent \textbullet\ \(f_t\) is the forget gate.\\
\indent \textbullet\ \(o_t\) is the output gate.\\
\includegraphics[scale=0.4]{images/Screenshot-from-2021-03-16-15-51-05.png}
\subsection{Seasonal AutoRegressive Integrated Moving Average with eXogenous
factors (SARIMAX)}\\
While ARIMA is famous, it has limitations in handling seasonal fluctuation and external factors, which are common in time series data. SARIMAX model is an abbreviation for Seasonal Autoregressive Integrated Moving Average or Seasonal ARIMA and belongs to the family of the ARIMA model. It is mentioned in that the SARIMAX model is capable of handling time series data that has a single variable along with seasonality. Apart from the hyperparameters for ARIMA, three different hyperparameters namely the AutoRegression (AR), Integrated (I), and Moving Average (MA) for the seasonal part of the series along with one parameter for the period of the seasonality. Just adding X (an exogenous variable) in the SARIMA model makes it SARIMAX. Sarimax is denoted as SARIMAX(p,d,q)(P,D,Q,S) where P,D,Q represents the element of AR, differencing, and MA components respectively, while S represents periodicity of the seasons.
\begin{flalign*}
& (1 - \sum_{i=1}^p \phi_i L^i)(1 - \sum_{i=1}^P \Phi_i L^{iS}) \times (1 - L)(1 - L^S)^D Y_t && \\
& = (1 + \sum_{i=1}^q \theta_i L^i)(1 + \sum_{i=1}^Q \Theta_i L^{iS}) \varepsilon_t + \beta X_t &&
\end{flalign*}
Where: \\
\indent \textbullet\ P: the order of auto-regressive.\\
\indent \textbullet\ Q: the order of moving average.\\
\indent \textbullet\ D: the degree of differencing.\\
\indent \textbullet\ S: the periodicity of the seasons.\\
\indent \textbullet\ \(\Theta\): the seasonal AR paramater.\\
\indent \textbullet\ \(\Phi\): the seasonal MA parameter.\\






\subsection{Recurrent Neural Network (RNN)}\\
Recurrent Neural Network(RNN) is a type of Neural Network where the output from the previous step is fed as input to the current step.
\subsection{Gated Recurrent Unit}
Gated Recurrent Units (GRUs) are a type of RNN that were introduced by in 2014 as an improvement over the traditional LSTM networks. Like LSTMs, GRUs are designed to be able to process input sequences of arbitrary length and maintain a state that encodes information about the past. However, unlike LSTMs, which use multiple gates and an internal memory cell to control the flow of information, GRUs use a single update gate to decide which information to retain and a reset gate to decide which information to discard.
\begin{align*}
        u_t &= \sigma(W_u [h_{t-1}, x_t]) \\
    r_t &= \sigma(W_r [h_{t-1}, x_t]) \\
    h_t &= (1 - u_t) \ast h_{t-1} + u_t \ast \tanh(W [r_t \ast h_{t-1}, u_t])
\end{align*}


Where: \\
\indent \textbullet \(u_t\) is the update gate.\\
\indent \textbullet \(r_t\) is the reset gate.\\
\indent \textbullet \(h_t\) is the hidden state at time step t .\\
\includegraphics[scale=1]{images/Structure-of-the-gated-recurrent-unit-GRU-recurrent-network.png}
\subsection{Stacking Model}
Stacking or stacked generalization is an ensemble machine learning algorithm. The main idea of stacking model is that we combine multiple weak models to enhance the accuracy of prediction. This algorithm consists of two parts: base model and meta model. Meta model use output from base model to produce the best result.\\ \\
\includegraphics[scale=0.3]{images/Architecture of stacking model.png}
\subsection{AutoRegressive Transformer}

\subsection{AutoRegressive Integrated
Moving Average (ARIMA)} 
ARIMA is a commonly used statistical/economtric model for forecasting time sereis data. The ARIMA model consists of three parts: auto-regressive (AR), integrated (I) and moving-average (MA).The integrated component represents
the amount of differencing required to transform the series data
into a stationary representation. The auto-regressive represents the relationship between the present value of a time and its previous values, capturing their correlation. 
\begin{equation*}
(1 - \sum_{k=1}^{p} \alpha_k L^k)(1 + L^d)X_t = (1 - \sum_{k=1}^{q} \beta_k L^k) \varepsilon_t
\end{equation*}
Where:\\
\indent \textbullet \(X_t\) is the time series.\\
\indent \textbullet \(\alpha_k\) are the parameters of autoregressive parts. \\
\indent \textbullet \(\beta_k\) are the parameters of the moving average part.\\
\indent \textbullet \(\varepsilon_t\) is white noise error.\\
\indent \textbullet \(p\) is the order of auto-regressive part.\\
\indent \textbullet \(q\) is the order of the moving part.\\
\indent \textbullet \(d\) is the degree of differencing.\\

Let L be the lag operator, in the above equation and p,d,q are hyper-parameters over which we optimized.At each time t, we train a model using the price history to predict the price at time t and use the sign of the change in price as a prediction.
\subsection{Linear Regression}
 Linear regression is a statistical model which estimates the linear relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables)
 \[Y=\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_kX_k+\varepsilon\]
Where:\\
	\indent\textbullet\ Y is the dependent variable (Target Variable).\\
	\indent\textbullet\ \(X_1, X_2, \ldots, X_k\) are the independent (explanatory) variables.\\
	\indent\textbullet\ \(\beta_0\) is the intercept term.\\
	\indent\textbullet\ \(\beta_1,..., \beta_k\) are the regression coefficients for the independent variables.\\
	\indent\textbullet\ \(\varepsilon\) is the error term.
 \subsection{Preprocessing}
 \textbullet\ data transform \\
 \textbullet\ data scaling
 \subsection{Look-Back window}
\section{Result}

\subsection{Evaluation Methods}
\textbf{Mean absolute error} (MAE): 
The Mean Absolute Error is the average of all absolute errors. The formula is:\\
\[MAE = \frac{1}{n} \sum_{i=1}^{n} | y_i - \hat{y}_i |\]\\
  
\textbf{Root mean squared error} (RMSE): The Root Mean Absolute Error is the standard deviation of the residuals (prediction errors). The formula is: \\
\[RMSE=\sqrt{\sum_{i=1}^{n} \frac{(\hat{y_i}-y_i )^2}{n} }\]\\
\textbf{Mean Absolute Percentage Error} (MAPE): The Mean Absolute Percentage Error is a measure of prediction accuracy of a forecasting method. The formula is:  \\
\[MAPE=\frac{1}{n}\sum_{i=1}^{n} \frac{|y_i-\hat{y}_i|}{y_i}\]
Where: \\
	\indent\textbullet\ \(n\) is the number of observations in the dataset.\\
	\indent\textbullet\ \(y_i\)  is the true value.\\
	\indent\textbullet\ \(\hat{y_i}\) is the predicted value.
 \subsection{Bitcoin}
\subsection{Ethereum}
\subsection{Binance}
\section{Conclusion}
\subsection{Summary}

\subsection{Future Considerations}

\section*{Acknowledgment}
\addcontentsline{toc}{section}{Acknowledgment}
\nocite{*}
\bibliographystyle{IEEEannot}
\bibliography{Ref}
\EOD

\end{document}